#importing all required packages
import tensorflow as tf
from keras.models import Sequential
import pandas as pd
from keras.layers import Dense
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
import math
import random
import time
import matplotlib.pyplot as plt


#We define the function we will use to calculate the Mobius Function for a given integer (this is just a brute force methodology)
def MobiusFunction(n):
    i = 2
    factors = []
    while i * i <= n:
        if n % i:
            i += 1
        else:
            if (n % (i * i) == 0):
                return 0
            n //= i
            factors.append(i)
    if n > 1:
        factors.append(n)
    return ((-1) ** len(factors))
    
    
#We define the function that finds the prime factorizations of an integer N
def PrimeFactorization(n):
    i = 2
    factors = []
    while i * i <= n:
        if n % i:
            i += 1
        else:
            n //= i
            factors.append(i)
    if n > 1:
        factors.append(n)
    return factors
    
    
#We define the function for generating a data set based on a "k" value that we have 2^k to 2^(k+1)
def dataSetGeneration(k):
    #We start by making an empty array (matrix) which will end up being out data. 
    #The first column of the array will be our label, the rest will be features
    dataSet = []
    #We set our bounds [a, b) for the integers we will look at
    a = 2**k
    b = 2*a + 1
    #We construct the data set array by compiling the data for each integer in our range then adding it to the data set as a new row
    for i in range(a, b):
        N = []
        #First piece of data is the Mobius value of the integer (what we will eventually try to predict)
        N.append(MobiusFunction(i))
        #Next we add some features
        #We add the integer itself
        N.append(i)
        #integer value modulo 4
        N.append(i%4)
        #integer value modulo 9
        N.append(i%9)
        #integer value modulo 25
        N.append(i%25)
        #integer value modulo 49
        N.append(i%49)
        #largerst prime factor
        ## N.append(max(PrimeFactorization(i)))
        #Number of prime factors
        N.append(len(PrimeFactorization(i)))
        dataSet.append(N)
    return dataSet
    
    
#Generates a list of the prime numbers from 2 up to N
def PrimeNumbersUpTo(n):
    primes = list(range(2,n+1))
    end = len(primes)
    j = 0
    while j < end - 1:
        i = 0
        while i < end:
            minPrime = primes[j]
            if ((primes[i] % minPrime == 0) and (primes[i] != minPrime)):
                primes.pop(i)
                end = len(primes)
            i += 1
        j += 1
    return primes
    
    
#Using a list of prime numbers, generates a list of those primes, pairs of those primes multiplied together, and sets of 3 multiplied together
#NOTE: No repeated prime in a set (i.e. we have 2*3 but not 2*2)
def DataGenPrimeList(primes):
    entireDataSet = [[i] for i in primes]
    finalData = []
    for a in entireDataSet:
        remainingElements = [i for i in primes]
        if (len(a) == 3):
            continue
        b = 0
        end = len(remainingElements)
        while b < end:
            c = []
            if ((remainingElements[b] not in a) and (remainingElements[b] > max(a))):
                c = [i for i in a]
                c.append(remainingElements[b])
                entireDataSet.append(c)
                remainingElements.pop(b)
                b = 0
            end = len(remainingElements)
            b += 1
    for d in entireDataSet:
        result = 1
        for i in d:
            result = result * i
        finalData.append(result)
    return finalData
    
    
#Generates a data set based on the set of primes generated by the DataGenPrimeList function using the same features as before
def DataSetBasedOnPrimes(dataSetPrime):
    dataSet = []
    for i in dataSetPrime:
        N = []
        #First piece of data is the Mobius value of the integer (what we will eventually try to predict)
        N.append(MobiusFunction(i))
        #Next we add some features
        #We add the integer itself
        N.append(i)
        #integer value modulo 4
        N.append(i%4)
        #integer value modulo 9
        N.append(i%9)
        #integer value modulo 25
        N.append(i%25)
        #integer value modulo 49
        N.append(i%49)
        #largerst prime factor
        ## N.append(max(PrimeFactorization(i)))
        #Number of prime factors
        ## N.append(len(PrimeFactorization(i)))
        dataSet.append(N)
    return dataSet
    
    
#The following function takes in two inputs, the entire data set, and the percent we want to use to train. 
#Then, it finds the category (0, 1, -1) with the smallest number of elements in our data set. 
#Finally, it takes a number of random elements from each category equal to the number in the smallest category multiplied by our percent
def trainingDataSetPerc(dataSetArray, percent):
    #First we convert the input percent into a decimal
    n = (percent/100)
    #Next we count the number of elements in each category (0, 1, -1)
    count_0 = 0
    count_1 = 0
    count_neg1 = 0
    #We iterate through our entire data set and for each element we add to the count of its respective Mobius Function
    for a in dataSetArray:
        if a[0] == 0:
            count_0 += 1
        if a[0] == 1:
            count_1 += 1
        if a[0] == -1:
            count_neg1 += 1
    
    #We take the number in the category with the fewest elements
    #We make sure that no category has 0 elements and if it does we look at the the next smallest category
    i = [j for j in [count_0, count_1, count_neg1] if (j != 0)]
    minimum = min(i)
    
    #We get the integer closest to the number of elements in the smallest category multiplied by the percent we want to train on
    num = int(minimum * n)
    #Now we actually distribute the data
    #First we break up our entire data set into three sets for each respective Mobius Function result
    zerosarray = []
    onesarray = []
    negonesarray = []
    for i in dataSetArray:
        if i[0] == 0:
            zerosarray.append(i)
        elif i[0] == 1:
            onesarray.append(i)
        else:
            negonesarray.append(i)

    #Now we making our training data by picking a random assorment of elements from each of our three data sets equal to the number we calculated earlier
    trainingData = []
    if (count_0 != 0):
        for j in random.sample(zerosarray, num):
            trainingData.append(j)
    if (count_1 != 0):
        for j in random.sample(onesarray, num):
            trainingData.append(j)
    if (count_neg1 != 0):
        for j in random.sample(negonesarray, num):
            trainingData.append(j)
    return trainingData
    
    
#The following function makes the test data set by taking all the elements in the original data set that don't show up in the training data set
def testDataSet(dataSetArray, trainingDataNP, k):
    alreadyHere = {}
    for i in range(2**k, 2*2**k + 1):
        alreadyHere[i] = False
    for j in trainingDataNP:
        alreadyHere[j[1]] = True
    testData = []
    for i in dataSetArray:
        if (alreadyHere[i[1]]):
            continue
        else:
            testData.append(i)
    return testData
    
    
#This is a much much cleaner function that partitions the test data based on the training data 
def testDataSetBetter(dataSetArray, trainingDataNP):
    return [i for i in dataSetArray if (i[1] not in trainingDataNP[:,1])]
    
    
#The following function converts the labels column in the inputted data set into a one-hot vector and outputs an m x 1 matrix with just the labels
def labels(data):
    #Create an m x 1 column matrix corresponding to the first (label) column of the input data set
    lab = data[:, 0]
    #Create a new matrix in which each row is going to be the one-hot vector associated with the label from the input data set
    label = []
    #Iterate through our label column and for each element add a new row to our empty matrix corresponding to the one-hot encoding
    for a in lab:
        if (a == 0):
            label.append(np.asarray([0, 1, 0]))
        elif (a == 1):
            label.append(np.asarray([1, 0, 0]))
        else:
            label.append(np.asarray([0, 0, 1]))
    return np.asarray(label)
    
    
#The following function is our default prediction algorithm. 
#This will predict a mobius function value of 0 for any integer divisible by 4, 9, 25, or 49. 
#It predits a mobius function value of 1 for all the rest of the integers.
def defaultPred(dataSetArray):
    pred = []
    for a in dataSetArray:
        if a[1]%4 == 0:
            pred.append(0)
            continue
        if a[1]%9 == 0:
            pred.append(0)
            continue
        if a[1]%25 == 0:
            pred.append(0)
            continue
        if a[1]%49 == 0:
            pred.append(0)
            continue
        else:
            pred.append(1)
    return pred
    
    

#We have two methods to set up the dataset currently I am looking at the method for 2^k to 2^(k+1). 
#If we wanted to look at the prime numbers method, you can comment out the code here and uncomment the code below.
#DATA SET USING 2^k to 2^(k+1)

#     Creating the data for the model to train on
dataSet = dataSetGeneration(16)
#     Partitioning the training data
trainingData = np.asarray(trainingDataSetPerc(dataSet, 70))

#     Partitioning the test data based on the training data (this uses the old function which is not good)
#     testData = np.asarray(testDataSet(dataSet, trainingData, 16))

#     Partitioning the test data based on the training data (this uses the new function which is much better)
testData = np.asarray(testDataSetBetter(dataSet, trainingData))


#DATA SET USING PRIME NUMBERS UP TO N

#     Creating the data for the model to train on
#dataSet = DataSetBasedOnPrimes(DataGenPrimeList(PrimeNumbersUpTo(400)))
#     Partitioning the training data
#trainingData = np.asarray(trainingDataSetPerc(dataSet, 50))

#     Partitioning the test data based on the training data (this uses the old function which is not good)
#     testData = np.asarray(testDataSet(dataSet, trainingData, 16))

#     Partitioning the test data based on the training data (this uses the new function which is much better)
#testData = np.asarray(testDataSetBetter(dataSet, trainingData))


#X and y are the features and labels of the training data respectively. 
#Then we split the training data into 80 percent to be trained on and 20 percent to act as the validation set (X_test and y_test)
X = trainingData[:, 1:]
y = labels(trainingData)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)


#Determines which features are most relavent (the printed list shows the important indicies of features from most to least)
X_indices = np.arange(X.shape[-1])
selector = SelectKBest(f_classif, k='all')
selected_features = selector.fit_transform(X, trainingData[:, 0])
f_score_indexes = (-selector.scores_).argsort()[:6]
print (f_score_indexes)
scores = selector.scores_
scores /= scores.max()
#plots a visualization of the most important features
plt.bar(X_indices, scores, width=.2)


#Actual_X_test and Actual_y_test are the features and labels, respectively, of the test data the model hasn't seen yet
#These values will be used to test the accuracy of the model
Actual_X_test = testData[:, 1:]
Actual_y_test = labels(testData)


#create model (this can be played around with to try and create a better model)
input_shape_number = len(trainingData[0]) - 1
model = Sequential()
#Add layers to the model, each with a specified activation function (all added some dropout layers to help with over-fitting)
model.add(Dense((input_shape_number - 1), activation='relu', input_shape=(input_shape_number,)))
#model.add(Dense(5, activation='relu'))
#model.add(Dense(5, activation='relu'))
model.add(Dense(4, activation='relu'))

model.add(tf.keras.layers.Dropout(0.4))

model.add(Dense(4, activation='relu'))

model.add(tf.keras.layers.Dropout(0.4))

model.add(Dense(3, activation='sigmoid'))
model.add(Dense(3, activation='softmax'))


#compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
#print out a summary of the layers of the model
model.summary()


#normalize data
maximums = np.max(X_train, axis=0)


#Train the model. The start and time.time() are used to print out the duration it takes the model to run. 
#Alternatively, you can change verbose to equal 1 and that will show a progress bar for each epoch.
#There are two model.fit calls here. The first is commented out and uses a specific number of epochs. 
#The second is the one I am currently using and stops fitting the model once loss has remained the same for 5 epochs (the patience of the callback)
start = time.time()
#model.fit(X_train/maximums, y_train, epochs=500, batch_size=128, verbose=0,
#          validation_data = (X_test/maximums, y_test))
es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

model.fit(X_train/maximums, y_train, batch_size=32, verbose=0, validation_data = (X_test/maximums, y_test), callbacks=[es_callback])
print("Duration: ", time.time() - start)


#Making predictions with the model and testing accuracy on the test data
#The score shows the [loss, accuracy] of the model
y_pred = model.predict(Actual_X_test/maximums)
score = model.evaluate(Actual_X_test/maximums, Actual_y_test, verbose=1)
print(score)


#Using the default prediction algorithm to test our model vs. a basic prediction algorithm
y_default = defaultPred(testData)
#Calculating the percent correctly predicted via the default predict, by count the number of correct predictions 
count = 0
count_correct = 0
for a in testData:
    if (a[0] == y_default[count]):
        count_correct += 1
    count += 1
print (count_correct/count*100, "%")


#Calculating the percentage of -1s in our test data
count = 0
count_correct = 0
for a in testData:
    if (a[0] == -1):
        count_correct += 1
    count += 1
print (count_correct/count*100, "%")
